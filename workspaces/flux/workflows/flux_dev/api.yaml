name: Create an image (Advanced)
description: Generate an image from text using Flux.
tip: |-
  Flux is a text to image model by Black Forest Labs (ex StabilityAI employees ) that excels at prompt coherence, aesthetics and text generation. This tool is a strong choice for generating an image from text! Flux-dev can use ControlNet guidance to maintain the contours or shape outlines of the input image while generating a new style or aesthetic on top of it, and LoRA models to apply a pretrained fine tuned concept. While controlnet exists for this tool, the controlnet models released so far are considered inferior to those from SDXL, and the other tools should be preferred for shape guidance creations. Flux-dev was trained on a dataset ranging in size from .1 to 2 megapixels, and is capable of generating images at higher resolutions than Stable Diffusion without noticable artifacting. While best practice seems to dictate that Flux likes similar resolutions to Stable Diffusion (such as those in the resolutions: list), it is capable of good results higher resolutions, rounded to 64 pixels such as 1920x1088, 1408x1408, 1728x1152, 1664x1216 etc.
thumbnail: app/flux.png
cost_estimate: 2 * n_samples
output_type: image
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 1-1_1024x1024, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
handler: comfyui
status: prod
base_model: flux-dev
comfyui_output_node_id: 206
comfyui_intermediate_outputs:
  controlnet_guidance: 342
parameters:
  prompt:
    type: string
    label: Prompt
    description: Describe an image
    tip: |-
      Flux loves very detailed and descriptive prompts so try to be elaborate. Flux is also great at drawing text in images when requested. Flux uses a T5 text encoder in addition to CLIP, which allows for more conversational human-like natural language prompting. Create great T5 text encoder prompts for Flux-dev by specifying the subject, style, composition, lighting, color palette, mood/atmosphere, technical details, and additional elements to guide the generation of detailed and cohesive visual descriptions.
    required: true
    default: null
    comfyui:
      node_id: 131
      field: inputs
      subfield: value
  width:
    type: integer
    label: Width
    description: Width in pixels
    required: true
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 119
      field: inputs
      subfield: width
  height:
    type: integer
    label: Height
    description: Height in pixels
    required: true
    default: 1024
    minimum: 256
    maximum: 2048
    visible_if: use_init_image=false
    step: 8
    comfyui:
      node_id: 119
      field: inputs
      subfield: height
  use_init_image:
    type: boolean
    label: Add a starting image
    description: Enable image-to-image, and controlnet guidance features
    tip: |-
      Using a starting image helps guide your prompted creation towards the colors present in your initial input. If an image is supplied with the prompt or controlnet is employed, this should be true.
    default: false
    comfyui:
      node_id: 145
      field: inputs
      subfield: value
  init_image:
    type: image
    label: Starting image
    description: Initial image from which to start
    tip: |-
      This can be used to guide the image generation with colors and shapes of an input image.
    visible_if: use_init_image=true
    comfyui:
      node_id: 302
      field: inputs
      subfield: image
  denoise:
    type: float
    label: AI strength
    description: Strength of the AI generation process on top of the starting image
    tip: |-
      Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. You typically want to use this with high denoise values >0.8 for Flux unless you only want only very subtle variations of the starting image.
    default: 1.0
    minimum: 0.0
    maximum: 1.0
    visible_if: use_init_image=true
    comfyui:
      node_id: 323
      field: inputs
      subfield: value
  size_from_input:
    type: boolean
    label: Use starting image size
    description: |-
      Override the width and height parameters with the resolution of your starting image.
    tip: |-
      It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution, but if you need to match the exact image dimensions of your input, this should be true. Keep in mind that all diffusion models will enforce an 8 pixel alignment, so if your input image has a dimension that is not divisible by 8, it will be rounded to the nearest 8 pixel value.
    default: false
    visible_if: use_init_image=true
    comfyui:
      node_id: 120
      field: inputs
      subfield: value
  use_controlnet:
    type: boolean
    label: Use starting image as shape guide
    description: Activate Controlnet to use the starting image shape in the generation.
    tip: Controlnet guides the generated output towards the shape of the Starting Image.
    default: false
    comfyui:
      node_id: 133
      field: inputs
      subfield: value
  preprocessor:
    type: string
    label: Controlnet preprocessor
    description: Determines the type of shape guidance controlnet model used
    tip: |-
      Help the user determine the best preprocessor for their input image. CannyEdgePreprocessor detects thin binary edges and is useful for tasks requiring fast edge detection, such as architectural designs or simple object outlines, but it may struggle with complex scenes or noise. MiDaS-DepthMapPreprocessor generates luminance-based depth maps and is ideal for creating realistic 3D-like compositions, such as emphasizing depth in landscapes or isolating foreground objects. HEDPreprocessor excels at producing smooth, intricate edge detection and is suitable for detailed illustrations, artistic line work, or noise-sensitive scenes. DWPreprocessor, as an OpenPose-based preprocessor, identifies human keypoints and is perfect for generating images with accurate human poses, such as action shots, dance scenes, or character positioning. AnyLineArtPreprocessor_aux converts images into detailed line drawings and is best for black-and-white artwork, manga-style illustrations, or clear contrast outlines. ScribblePreprocessor processes rough sketches into structured inputs, ideal for refining rough concepts or integrating hand-drawn elements into finished compositions with a sketch-like aesthetic.
    visible_if: use_controlnet=true
    default: CannyEdgePreprocessor
    choices: [CannyEdgePreprocessor, MiDaS-DepthMapPreprocessor, AnyLineArtPreprocessor_aux, HEDPreprocessor, DWPreprocessor, ScribblePreprocessor]
    choices_labels: [edges (canny), depth, lines (lineart), soft lines (HED), human pose, scribble]
    comfyui:
      node_id: 104
      field: inputs
      subfield: preprocessor
  controlnet_strength:
    type: float
    label: Controlnet strength
    description: set the guidance strength of the controlnet model in slot 1
    tip: |-
      A good default for flux is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for more heavy handed results.
    default: 0.6
    minimum: 0.0
    maximum: 1.5
    visible_if: use_controlnet=true
    comfyui:
      node_id: 135
      field: inputs
      subfield: value
  use_lora:
    type: boolean
    label: Use a trained model
    description: Apply LoRA finetune model style to image generation
    tip: |-
      Models created with Eden LoRA trainer can add people, styles and conceptual embeddings into the diffusion model, giving it an idea of new information provided by the user.
    default: false
    comfyui:
      node_id: 144
      field: inputs
      subfield: value
  lora:
    type: lora
    label: Trained model (LoRA)
    description: Use a LoRA finetune on top of the base model.
    visible_if: use_lora=true
    comfyui:
      node_id: 80
      field: inputs
      subfield: lora_name
  lora_strength:
    type: float
    label: Model Strength
    description: Strength of the custom model (LoRA). Decrease this value to improve prompt adherence. Increase to get more of the LoRA style.
    tip: |-
      If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength. A stronger LoRA strength around .9 is better to recreate faces when using a model trained on specific person.
    default: 0.8
    minimum: 0.0
    maximum: 1.25
    step: 0.01
    visible_if: use_lora=true
    comfyui:
      node_id: 141
      field: inputs
      subfield: value
  use_lora2:
    type: boolean
    label: Use a trained model 2
    description: Apply LoRA finetune model style to image generation
    tip: |-
      Models created with Eden LoRA trainer can add people, styles and conceptual embeddings into the diffusion model, giving it an idea of new information provided by the user.
    default: false
    comfyui:
      node_id: 339
      field: inputs
      subfield: value
  lora2:
    type: lora
    label: Trained model (LoRA) 2
    description: Use a second LoRA finetune on top of the base model.
    visible_if: use_lora2=true
    comfyui:
      node_id: 337
      field: inputs
      subfield: lora_name
  lora2_strength:
    type: float
    label: Model Strength 2
    description: Strength of the custom model (LoRA). Decrease this value to improve prompt adherence. Increase to get more of the LoRA style.
    tip: |-
      If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength. A stronger LoRA strength around .9 is better to recreate faces when using a model trained on specific person.
    default: 0.8
    minimum: 0.0
    maximum: 1.25
    step: 0.01
    visible_if: use_lora2=true
    comfyui:
      node_id: 338
      field: inputs
      subfield: value
  use_style_image:
    type: boolean
    label: Use a style image
    description: Apply a style image to the generation
    tip: |-
      Style images can be used to guide the color palette, texture, and overall style of the generated image. This can be useful for creating images with a specific look or feel, such as a painting or photograph. This implementation uses the Flux Redux model to transfer traits of the style image to the output creation.
    default: false
    comfyui:
      node_id: 396
      field: inputs
      subfield: value
  style_image:
    type: image
    label: Style image
    description: Image to use as a style reference
    tip: |-
      This can be used to guide the color palette, texture, and overall style of the generated image.
    visible_if: use_style_image=true
    comfyui:
      node_id: 142
      field: inputs
      subfield: image
  style_strength:
    type: float
    label: Style strength
    description: Strength of the style image
    tip: |-
      A good default for Flux Redux is around 0.75, with ranges between 0.5 for subtle guidance, and 0.9 for more heavy handed results. This parameter is very sensative, and small changes can have a large impact on the output. The prompt length, source image, and other factors like custom model lora strength can all affect the optimal value for this parameter.
    default: 0.75
    minimum: 0.0
    maximum: 1.0
    step: 0.01
    visible_if: use_style_image=true
    comfyui:
      node_id: 381
      field: inputs
      subfield: value  
  seed:
    type: integer
    label: Seed
    description: Set random seed for reproducibility. If blank, will be set to a random value.
    tip: |-
      You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank!
    default: random
    minimum: 0
    maximum: 2147483647
    comfyui:
      node_id: 330
      field: inputs
      subfield: seed
  n_samples:
    type: integer
    label: Number of samples
    description: Number of samples to generate
    tip: |-
      This is the number of outputs generated for this prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
    default: 1
    minimum: 1
    maximum: 4
  steps:
    type: integer
    label: Steps
    description: Number of steps to take in the diffusion process
    tip: |-
      The number of steps taken in the diffusion process. Higher values will result in more detailed images, but will also take longer to generate.
    default: 25
    minimum: 1
    maximum: 35
    comfyui:
      node_id: 393
      field: inputs
      subfield: steps
  flux_guidance:
    type: float
    label: Prompt Conditioning
    description: Strength of the prompt and image conditiong influence on the creation result.
    tip: |-
      This parameter controls the influence of propmt conditioning guidance on image generation. For stylized images a higher value of 3.5-4 may be desirable, and for realistic images closer to 2 is a good value, with an extreme minimum around 1.2. Higher values will more closely follow the input text and images.
    default: 3.5
    minimum: 1.0
    maximum: 4.0
    step: 0.01
    comfyui:
      node_id: 101
      field: inputs
      subfield: guidance
  include_caption:
    type: boolean
    label: Include caption
    description: Add the prompt as text on top of the image
    tip: |-
      This will add the prompt as a caption to the generated image.
    default: false
    hide_from_agent: true
    comfyui:
      node_id: 407
      field: inputs
      subfield: value