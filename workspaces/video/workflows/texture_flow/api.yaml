name: TextureFlow
description: Mix multiple images into a video, optionally add a driving motion or shape.
tip: |-
  This workflow creates animations from a set of images and several other inputs. First is the motion template that controls how the content of the style images is mapped onto the video (spatially). Second is an optional controlnet image (set use_controlnet to true) and preprocessor type that can add very strong and specific guidance to the generated video like embedding the contours of a logo or word into the video or eg applying the lines or perceived depth of a projection surface when doing videomapping.
thumbnail: app/style_mixing_opt.mp4
cost_estimate: 0.4 * n_frames * (width + height)/(2*640) * n_steps/8
output_type: video
resolutions: [16-9_928x512, 3-2_864x576, 1-1_640x640, 2-3_576x864, 9-16_512x928]
handler: comfyui
status: prod
base_model: sd15
comfyui_output_node_id: 446
comfyui_intermediate_outputs:
  control_signal_1: 501
  mapping_motion: 199
parameters:
  images:
    type: array
    items:
      type: image
    label: Style Images
    description: Style images driving the content of the generated video (through IP-adapter)
    min_length: 1
    max_length: 6
    required: true
    comfyui:
      node_id: 74
      field: inputs
      subfield: folder
      preprocessing: folder
  n_frames:
    type: integer
    label: Frames
    description: |-
      Number of video frames to generate (8 frames = 1 second of video, although the final video will be interpolated to always be 24 fps)
    # required: true
    minimum: 16
    maximum: 192
    default: 24
    hide_from_agent: true
    step: 1
    comfyui:
      node_id: 510
      field: inputs
      subfield: value
  n_steps:
    type: integer
    label: Sampling steps
    description: |-
      Number of LCM diffusion steps. Lower is cheaper / faster (for experimenting with settings), higher can sometimes yield slightly better quality.
    default: 8
    minimum: 4
    maximum: 14
    step: 1
    comfyui:
      node_id: 524
      field: inputs
      subfield: value
  width:
    type: integer
    label: Width
    description: |-
      Width of the video diffusion in number of pixels. High values often create artefacts like multiple faces etc.
    default: 640
    minimum: 512
    maximum: 1280
    step: 32
    comfyui:
      node_id: 261
      field: inputs
      subfield: width
  height:
    type: integer
    label: Height
    description: |-
      Height of the video diffusion in number of pixels. High values often create artefacts like multiple faces etc.
    default: 640
    minimum: 512
    maximum: 1280
    step: 32
    comfyui:
      node_id: 261
      field: inputs
      subfield: height
  upscale:
    type: boolean
    label: Activate upscale
    description: |-
      Do an upscale pass at the end, increasing the output resolution of the animation to 1024. (Suggested to leave this off for cheaper and faster testing until you hit a sweet spot!)
    tip: |-
      This toggle can be activated to create a really good, HD animation once all the settings are dialed in!
    default: false
    comfyui:
      node_id: 415
      field: inputs
      subfield: value
  use_controlnet1:
    type: boolean
    label: Use controlnet
    description: Apply Controlnet shape guidance.
    tip: |-
      Controlnet uses image preprocessors to guide the output results towards the shape of a Guidance Image.
    default: false
    comfyui:
      node_id: 273
      field: inputs
      subfield: value
  control_input:
    anyOf:
      - type: image
      - type: video
    label: Guidance input
    description: |-
      A guidance image/GIF/video used as input to the controlnet. Will guide the shape of the output video.
    visible_if: use_controlnet1=true
    comfyui:
      node_id: 552
      field: inputs
      subfield: path
  preprocessor1:
    type: string
    label: Controlnet1 preprocessor
    description: |-
      Type of controlnet preprocessor. Examples can be seen at https://github.com/lllyasviel/ControlNet-v1-1-nightly?tab=readme-ov-file#controlnet-11-depth
    tip: |-
      depth will try to maintain the perceived depth of the input scene. Canny edge creates strong edges adhering to the shape of your image, whereas scribble will create guidance towards a rougher sketched shape of your starting image and often produces better quality video at the cost of less finegrained correspondence with the control image. Pose will try to extract the pose from a person and inject it into the video.
    visible_if: use_controlnet1=true
    default: CannyEdgePreprocessor
    choices: [CannyEdgePreprocessor, DepthAnythingV2Preprocessor, AnyLineArtPreprocessor_aux, DensePosePreprocessor, Scribble_XDoG_Preprocessor, none]
    choices_labels: [Edges (Canny), Depth, Lineart, human pose, Scribble lines, Luminance (QR-code, Dark/bright patterns)]
    comfyui:
      node_id: 406
      field: inputs
      subfield: preprocessor
      remap:
      - node_id: 107
        field: inputs
        subfield: control_net_name
        map:
          AnyLineArtPreprocessor_aux: control_v11p_sd15_lineart.pth
          CannyEdgePreprocessor: control_v11p_sd15_canny.pth
          DensePosePreprocessor: control_v11p_sd15_openpose.pth
          DepthAnythingV2Preprocessor: control_v11f1p_sd15_depth.pth
          Scribble_XDoG_Preprocessor: control_v11p_sd15_scribble.pth
          none: controlnetQRPatternQR_v2Sd15.safetensors
  controlnet_strength1:
    type: float
    label: Controlnet1 strength
    description: |-
      set the guidance strength of the first controlnet model, recommended values are 0.4-0.6
    tip: |-
      A good default is around 0.5, with ranges between 0.35-0.5 for subtle guidance, and 0.5-0.8 for something more heavy handed
    default: 0.5
    minimum: 0.0
    maximum: 1.5
    step: 0.01
    visible_if: use_controlnet1=true
    comfyui:
      node_id: 116
      field: inputs
      subfield: strength
  control_input_fit_strategy:
    type: string
    label: Fit strategy
    description: |-
      Determines how your guidance input is resized to the target width/height of the generation
    tip: |-
      fill / crop will cut off edges, stretch will distort the control_input and pad can cause strong visual artefacts (not recommended).
    visible_if: use_controlnet1=true
    default: fill / crop
    choices: [stretch, fill / crop, pad]
    comfyui:
      node_id: 262
      field: inputs
      subfield: method
  mapping_mode:
    type: string
    label: Content Mapping Mode
    description: Mapping mode that controls how the style images will get mapped into the video
    choices: [concentric_circles_inwards, concentric_circles_outwards, concentric_rectangles_inwards, concentric_rectangles_outwards, rotating_segments_clockwise, rotating_segments_counter_clockwise, pushing_segments_clockwise, pushing_segments_counter_clockwise, vertical_stripes_left, vertical_stripes_right, horizontal_stripes_up, horizontal_stripes_down]
    default: concentric_circles_outwards
    comfyui:
      node_id: 536
      field: inputs
      subfield: mode
  motion_scale:
    type: float
    label: Motion Scale
    description: |-
      Motion scale for the animation, determines how much animated motion will be in the final video. Recommended to leave this at default.
    tip: |-
      The default value of 1.15 is good for most cases. Lowering to eg 0.9 will result in very subtle texture motion, increasing to eg 1.25 will result in more motion. Going above 1.3 is almost never desirable.
    default: 1.15
    minimum: 0.7
    maximum: 1.4
    step: 0.01
    comfyui:
      node_id: 450
      field: inputs
      subfield: float_val
  seed:
    type: integer
    label: Seed
    description: Set random seed for reproducibility. If blank, will be set to a random value.
    tip: |-
      You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this at default!
    default: random
    minimum: 0
    maximum: 2147483647
    default: random
    comfyui:
      node_id: 282
      field: inputs
      subfield: seed
