name: Create an image (SDXL)_test
description: Generate an image from text using our classic SDXL model and trained concepts.
thumbnail: app/txt2img.png
tip: This is the most basic and common function which generates an image from text a prompt, with ControlNet guidance to maintain the contours or shape outlines of the input image while generating a new style or aesthetic on top of it, LoRA models to apply a pretrained fine tuned concept, and ipAdapter functionality to transfer the style from a Style Image to the output result.
output_type: image
status: staging
private: true
visible: false
cost_estimate: 1 * n_samples
handler: comfyui
resolutions: [21-9_1536x640, 16-9_1344x768, 3-2_1216x832, 4-3_1152x896, 5-4_1088x896, 1-1_1024x1024, 4-5_896x1088, 3-4_896x1152, 2-3_832x1216, 9-16_768x1344, 9-21_640x1536]
comfyui_output_node_id: 146
comfyui_intermediate_outputs:
  - name: controlnet_signal
    node_id: 323
parameters:
- name: prompt
  label: Prompt
  description: Describe an image
  tip: Prompts are specific, detailed, concise, and visually descriptive, avoiding unnecessary verbosity and abstract, generic terms. They usually have ● Primary subject (i.e., person, character, animal, object, ...), e.g "Renaissance noblewoman", "alien starship". Should appear early in prompt. ● Action of the subject, e.g. "holding an ancient book", "orbiting a distant planet", if there is no main subject, a good context description will do here (what is going on in the scene?). They sometimes have several stylistic modifiers near the end of the prompt, such as ● Background, environment or context surrounding the subject, e.g. "in a dimly lit Gothic castle", "in a futuristic 22nd century space station". ● Secondary items that enhance the subject or story. e.g. "wearing an intricate lace collar", "standing next to a large, ancient tree". ● Color schemes, e.g. "shades of deep red and gold", "monochrome palette with stark contrasts", "monochrome", ... ● Style or method of rendering, e.g. "reminiscent of Vermeer's lighting techniques", "film noir", "cubism", ... ● Mood or atmospheric quality e.g. "atmosphere of mystery", "serene mood". ● Lighting conditions e.g. "bathed in soft, natural window light", "dramatic shadows under a spotlight". ● Perspective or or viewpoint, e.g. "bird's eye view", "from a low angle", "fisheye", .. ● Textures or materials, e.g. "textures of rich velvet and rough stone". ● Time Period, e.g. "Victorian Era", "futuristic 22nd century". ● Cultural elements, e.g. "inspired by Norse mythology", "traditional Japanese setting". ● Artistic medium, e.g. "watercolor painting", "crisp digital Unreal Engine rendering", "8K UHD professional photo", "cartoon drawing", ... They often end with trigger words that improve images in a general way, e.g. "High Resolution", "HD", "sharp details", "masterpiece", "stunning composition", ... If the prompt contains a request to render text, enclose the text in quotes, e.g. A Sign with the text “Peace”. IMPORTANT, If the user gives you a short, general, or visually vague prompt, you should augment their prompt by imagining richer details, following the tips above. If a user gives a long, detailed, or well-thought out composition, or requests to have their prompt strictly adhered to without revisions or embellishment, you should adhere to or repeat their exact prompt. The goal is to be authentic to the user's request, but to help them get better results when they are new, unsure, or lazy.
  type: string
  required: true
  comfyui: 
    node_id: 370
    field: inputs
    subfield: body
- name: no_token_prompt
  label: Prompt without the added embedding trigger.
  description: Base user prompt without the embedding token
  type: string
  hide_from_agent: true
  hide_from_ui: true
  comfyui: 
    node_id: 243
    field: inputs
    subfield: value
- name: n_samples
  label: Number of samples
  description: Number of samples to generate
  tip: This is the number of tries to generate for the prompt. If you get a request for n_samples > 1, you are still using a *single* prompt for the whole set.
  type: int
  required: true
  default: 1
  minimum: 1
  maximum: 4
- name: use_init_image
  label: Use starting image
  description: Enable image-to-image, and controlnet guidance features
  tip: Using a starting image helps guide your prompted creation towards the elements present in your initial input. if an image is supplied with the prompt or controlnet is employed, this should be true.
  type: bool
  default: false
  comfyui: 
    node_id: 198
    field: inputs
    subfield: value
- name: init_image
  label: Starting image
  description: Initial image from which to start diffusion process
  type: image
  visible_if: use_init_image=true
  comfyui: 
    node_id: 16
    field: inputs
    subfield: image
- name: size_from_input
  label: Use starting image size
  description: Override the width and height parameters with the actual resolution of your starting image. 
  tip: It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution.
  type: bool
  default: true
  visible_if: use_init_image=true
  comfyui: 
    node_id: 8
    field: inputs
    subfield: value
- name: enforce_SDXL_resolution
  label: Use optimized resolution
  description: Round to the nearest optimized SDXL resolution. 
  tip: It's best practice to use resolutions included in the models training data. This option will fill or crop the input image to a legal SDXL resolution.
  type: bool
  default: false
  visible_if: use_init_image=true
  comfyui: 
    node_id: 337
    field: inputs
    subfield: value
- name: denoise
  label: Generation strength
  description: Strength of the generation process on top of the starting image
  tip: Decreasing generation strength increases the influence of a starting image in image-to-image workflows. The default of 1 is 100% AI creativity, ignoring all traces of the starting image, whereas a medium blend of about 50% will maintain close adherance to the original input. Denoise should always stay at 1 unless an init image is employed; never use less than 1 unless you're specifically trying to do an img2img task. In this scenario, init_image should be true, and a non-default image should be supplied
  type: float
  default: 1.0
  minimum: 0.0
  maximum: 1.0
  visible_if: use_init_image=true
  comfyui: 
    node_id: 220
    field: inputs
    subfield: value
- name: use_lora
  label: Use LoRA
  description: Apply LoRA finetune model style to image generation
  tip: Models created with Eden LoRA trainer can add people, styles and conceptual embeddings into the diffusion model, giving it an idea of new information provided by the user.
  type: bool
  default: false
  comfyui: 
    node_id: 193
    field: inputs
    subfield: value
- name: lora
  label: LoRA
  description: Use a LoRA finetune on top of the base model.
  type: lora
  visible_if: use_lora=true
  comfyui: 
    node_id: 162
    field: inputs
    subfield: lora_name
- name: lora_strength
  label: LoRA strength
  description: Strength of the LoRA
  tip: If outputs resemble the LoRA but have low prompt adherence or all look the same, turn down the LoRA strength.
  type: float
  default: 0.5
  minimum: 0.0
  maximum: 1.5
  visible_if: use_lora=true
  comfyui: 
    node_id: 96
    field: inputs
    subfield: value
- name: lora_prompt
  label: LoRA prompt
  description: textual embedding token, invoked to trigger the LoRA
  type: string
  hide_from_agent: true
  hide_from_ui: true
  visible_if: use_lora=true
  comfyui: 
    node_id: 243
    field: inputs
    subfield: value
- name: use_controlnet
  label: Use controlnet
  visible_if: use_init_image=true
  description: Apply Controlnet guidance to the image generation.
  tip: Controlnet uses an assortment of image preprocessors to guide the output results towards the shape of a Starting Image.
  type: bool
  default: false
  comfyui: 
    node_id: 408
    field: inputs
    subfield: value
- name: preprocessor
  label: Controlnet preprocessor
  description: the preprocessor you choose will prepare your input image to guide your diffusion towards that shape
  tip:  depth will recreate an approximate depth of your input scene, and is best for 3 dimensional compositions. Canny edge creates strong edge detection adherance to the shape of your image, with cannyv2 as an updated model that's sharper but with less detail. The scribble model will create guidance towards a rougher sketched shape of your starting image. pose follows an open pose skeleton of a character's body position. threshold creates a hard black and white matte from the input image, and lineart provides a crisp hard edge guidance from the input. 
  type: string
  default: canny
  choices: [none, cannyv2, canny, depth, scribble, pose, threshold, lineart]
  visible_if: use_controlnet=true
  comfyui: 
    node_id: 283
    field: inputs
    subfield: select_item
- name: controlnet_strength
  label: Controlnet strength
  description: set the guidance strength of the controlnet model in slot 1
  tip: A good default is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for something more heavy handed
  type: float
  default: 0.7
  minimum: 0.0
  maximum: 1.5
  visible_if: use_controlnet=true
  comfyui: 
    node_id: 36
    field: inputs
    subfield: value
- name: use_ipadapter
  label: Use style image
  description: Transfer style from image
  tip: Ipadapter takes compositional concepts, subjects and aesthetic elements from a style image and applies it to the generated image.
  type: bool
  default: false
  comfyui: 
    node_id: 35
    field: inputs
    subfield: value
- name: style_image
  label: Style image
  description: Image of an aesthetic style, texture or visual content transferred to the generated image creation
  type: image
  visible_if: use_ipadapter=true
  comfyui: 
    node_id: 145
    field: inputs
    subfield: image
- name: ipadapter_strength
  label: Style strength
  description: set the strength of the image style transfer
  tip: Higher values (>0.8) will cause strong adherence to the style image, lower values (<0.3) will give more freedom to interpret the prompt.
  type: float
  default: 0.6
  minimum: 0.0
  maximum: 1.5
  visible_if: use_ipadapter=true
  comfyui: 
    node_id: 59
    field: inputs
    subfield: value
- name: ipadapter_mode
  label: Ipadapter style transfer mode
  description: Allows for different types of style transfer using ipadapter, each with different characteristics and more or less content blending.
  tip: All of these are variations of the same idea, but use different weighting schemas for injecting the style embedding into the unet. Each has different tradeoffs for style transfer versus content blending. Ease in-out is a great default, strong style transfer has less content bleeding.
  type: string
  visible_if: use_ipadapter=true
  default: strong style transfer
  choices: [ease in-out, style transfer, strong style transfer, style transfer precise, composition precise]
  visible_if: use_ipadapter=true
  comfyui: 
    node_id: 141
    field: inputs
    subfield: weight_type
- name: width
  label: Width
  description: Width in pixels
  tip: Default to using a high resolution of at least 1 megapixel for the image. Use landscape aspect ratio for prompts that are wide or more landscape-oriented, and portrait aspect ratio for tall things. When using portrait aspect ratio, do not exceed 1:1.5 aspect ratio. Only do square if the user requests it. Use your best judgment.
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 7
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height in pixels
  tip: Use the description for width to understand how to use height well.
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  visible_if: use_init_image=false
  step: 8
  comfyui: 
    node_id: 7
    field: inputs
    subfield: height
- name: negative_prompt
  label: Negative prompt
  description: Negative text prompt, what you do *not* want to generate.
  tip: Keep this at default or at most, add to this prompt, unless specifically instructed otherwise.
  type: string
  default: embedding:negativeXL_D, (worst quality, low quality, normal quality:1.5)
  comfyui: 
    node_id: 33
    field: inputs
    subfield: value
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank. Great for redoing an image generation you like with a slightly modified prompt, but otherwise you should usually leave this blank.
  type: int
  default: random
  minimum: 0
  maximum: 2147483647
  comfyui: 
    node_id: 27
    field: inputs
    subfield: seed
- name: steps
  label: Steps
  description: Set the number of diffusion steps
  tip: The default value is sufficient for most jobs, but at a speed cost can be increased, or decreased to undercook the image generation. 
  type: int
  default: 30
  step: 1
  minimum: 10
  maximum: 50
#  advanced: true
  comfyui: 
    node_id: 171
    field: inputs
    subfield: steps_total
- name: cfg
  label: Guidance
  description: CFG strength affects the influence of the prompt on the image generation.
  tip: Classifier Free Guidance strength should usually be set to the default, but can be increased or decreased within the range presented to increase or decrease the affect of the prompt on the creation.
  type: float
  default: 8.0
  step: 0.1
  minimum: 1.0
  maximum: 12.0
#  advanced: true
  comfyui: 
    node_id: 171
    field: inputs
    subfield: cfg