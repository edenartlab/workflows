name: Controlnet
description: Generate a new image which adheres to the shape or outline of an input image
tip: This tool is best for text-to-image when an input image is provided and the user wants to maintain the contours or shape outlines of the input image while generating a new style or aesthetic on top of it. A great example is re-texturizing a logo image or turning a photo of a face into a marble statue.
output_type: image
handler: comfyui
comfyui_output_node_id: 2
parameters:
- name: prompt
  label: Prompt
  description: Text prompt
  type: string
  required: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: positive
- name: negative
  label: Negative prompt
  description: Negative text prompt, what you do *not* want to generate.
  tip: Keep this at default or at most, add to this prompt, unless specifically instructed otherwise.
  type: string
  default:  
  comfyui: 
    node_id: 1
    field: inputs
    subfield: negative
- name: width
  label: Width
  description: Width in pixels
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  step: 8
  comfyui: 
    node_id: 1
    field: inputs
    subfield: width
- name: height
  label: Height
  description: Height in pixels
  type: int
  default: 1024
  minimum: 256
  maximum: 2048
  step: 8
  comfyui: 
    node_id: 1
    field: inputs
    subfield: height
- name: seed
  label: Seed
  description: Set random seed for reproducibility. If blank, will be set to a random value.
  tip: You should only set this if you want to start from/copy the seed of a previous image. Unless one is specified, you should leave this blank. Great for redoing an image generation you like with a slightly modified prompt, but otherwise you should usually leave this blank.
  type: int
  default: random
  minimum: 0
  maximum: 10000000000000000
  comfyui: 
    node_id: 1
    field: inputs
    subfield: seed
- name: init_image
  label: Starting image
  description: Initial image from which to start diffusion process
  type: image
  required: true
  comfyui: 
    node_id: 28
    field: inputs
    subfield: image
- name: denoise
  label: Generation strength
  description: Strength of the generation process on top of the starting image
  tip: Decreasing generation strength increases the influence of the starting image.
  type: float
  default: 1.0
  minimum: 0.0
  maximum: 1.0
  comfyui: 
    node_id: 11
    field: inputs
    subfield: denoise
- name: steps
  label: Steps
  description: Number of diffusion steps
  tip: Increasing the steps increases the quality of the image with diminishing returns beyond 35, as well as the run time. Only change this if you want to experiment with 'undercooking' the image.
  type: float
  default: 30
  minimum: 1
  maximum: 50
  comfyui: 
    node_id: 1
    field: inputs
    subfield: steps
- name: size_from_input
  label: Use input image size
  description: Override the width and height parameters with the actual resolution of your starting image. 
  tip: It's best practice to use resolutions included in the models training data. Upscaling later can get you to a higher resolution.
  type: bool
  default: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: size_from_input
- name: sdxl_aspect
  label: Use native aspect ratio
  description: Override the width and height parameters with the model's native training resolution. This will reduce artifacts.
  tip: SDXL was trained on images equaling about a megapixel, and generating within these preferred aspect ratios will produce better results
  type: bool
  default: true
  comfyui: 
    node_id: 1
    field: inputs
    subfield: sdxl_aspect
# - name: input_img_is_controlnet
#   label: Use starting image for controlnet
#   description: Use the input image as the controlnet guidance image
#   tip: Leave this as true unless you want to mix guidance from a different image while using an init with denoise < 1.0
#   type: bool  
#   default: true
#   comfyui: 
#     node_id: 1
#     field: inputs
#     subfield: input_img_is_controlnet
- name: preprocessor
  label: Preprocessor
  description: the preprocessor you choose will prepare your input image to guide your diffusion towards that shape
  tip:  depth_midas will recreate the approximate depth of your input scene, and is best for 3 dimensional compositions. canny creates strong edge adherance to the shape of your image, whereas scribble will create guidance towards a rougher shape of your input image. 
  type: string
  default: canny
  choices: [canny, depth_midas, scribble, dwpose]
  comfyui: 
    node_id: 100
    field: inputs
    subfield: preprocessor
- name: controlnet_strength
  label: Controlnet Strength
  description: set the guidance strength of the controlnet model in slot 1
  tip: A good default is around 0.6, with ranges between 0.2 for subtle guidance, and 1.0 for something more heavy handed
  type: float
  default: 0.6
  minimum: 0.0
  maximum: 1.5
  comfyui: 
    node_id: 100
    field: inputs
    subfield: strength
